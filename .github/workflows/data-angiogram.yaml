name: Data-Angiogram ğŸ«€ğŸ§ª
# Author: Vishnu Chanderraju
# Description: This workflow demonstrates how to run a data angiogram. It serves as template.
# This demonstration has been done for a presentation at smartdataconf.ru 2024 conference
# The talk is 'Assessing Data Pipeline Quality & Sanity with Data Angiograms'
# talk url: https://smartdataconf.ru/en/talks/124ab9dc49fa49ef8fc746eefecb9db1/?referer=%2Fen%2Fpersons%2Ff699b5d2a3514e6aaa50e92ece77f414%2F
# mit license

on:
  # Run this just before your STANDUP say 1100hrs EVERY DAY :)
  #schedule:
  #  - cron: "8 5 * * *" #just before standup at 1100am
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:

env:
  REGISTRY: docker.io/jaihind213
  POSTGRES_IMAGE: hbontempo/postgres-hll:15-alpine3.17-latest
  ZK_IMAGE: bitnami/zookeeper:3.5
  KAFKA_IMAGE: bitnami/kafka:3.2
  CONFLUENT_KAFKA_IMAGE: confluentinc/cp-kafka:6.1.1
  AIRFLOW_IMAGE: jaihind213/angiogram-airflow:4.0
  ANGIOGRAM_API_IMAGE: jaihind213/angiogram-api:4.1
  TAXI_TOPIC: taxi_rides


jobs:
  data-angiogram-pipeline:
    runs-on: ubuntu-latest
    environment: cicd
    steps:
    - name: Generate GitHub token
      run: |
        echo "Generate GitHub token using 'tibdex/github-app-token@v1' action"
    - uses: actions/checkout@v3
    - uses: actions/setup-python@v5
      with:
        python-version: '3.10'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} # required
    - name: â˜•  Set up JDK 17
      uses: actions/setup-java@v3
      with:
        java-version: '17.0'
        distribution: 'temurin'
    - name: ğŸ  let Pythons into garden
      run: |
        python -m pip install --upgrade pip
        pip install --upgrade poetry
        poetry install
        poetry export --without-hashes --format=requirements.txt > dags/requirements.txt 
        pip install -r dags/requirements.txt
        pip install parquet-cli
    - name: ğŸ”’  Login to Private docker registry ï¸
      run: |
        echo "Perform login to private docker registry using 'docker/login-action@v2' action"
    - name: ğŸ—ï¸  Pull Dockers & Record versions of docker images
      run: |
        chmod -R 777 ./airflow-data
        docker pull --quiet ${{ env.POSTGRES_IMAGE }}
        docker pull --quiet ${{ env.ZK_IMAGE }}
        docker pull --quiet ${{ env.KAFKA_IMAGE }}
        docker pull --quiet ${{ env.CONFLUENT_KAFKA_IMAGE }}
        docker pull --quiet ${{ env.AIRFLOW_IMAGE }}
        docker pull --quiet ${{ env.ANGIOGRAM_API_IMAGE }}
        sudo apt-get install -q=2 --yes --no-install-recommends postgresql-client >/dev/null 2>&1
        sudo apt-get install -q=2 --yes --no-install-recommends curl >/dev/null 2>&1 
        echo "listing images being used"
        docker images --digests
        #hardcoding ip of runner its always 172.17.0.1
        export HOST_IP=172.17.0.1
        echo "HOST_IP=$HOST_IP" >> $GITHUB_ENV
    - name: ğŸš§  Start Docker containers
      run: |
        docker network create angiogram
        docker compose up postgres -d;
        sleep 35;
        docker compose up zookeeper -d;
        sleep 35;
        docker compose up kafka -d;
        sleep 70;
        docker compose up init-kafka;
        docker compose up angiogram-api -d;
        sleep 35;
        docker compose up airflow-init;
        docker compose up airflow-scheduler airflow-webserver -d;
        sleep 420;
        docker compose logs
    - name: ğŸš’  Health Check Airflow
      run: |
        echo "checking if Airflow has Started properly"
        HOST_IP=${{ env.HOST_IP }}
        curl -I http://${HOST_IP}:8080/login/ |grep -i  "200"
    - name: ğŸ‡¨ğŸ‡­  Checkpoint 1 - Health Check Api
      run: |
        echo "checking if Api has Started properly"
        curl -I http://localhost:8000/foo/bar |grep -i  "404"
    - name: ğŸšš  Generate/Validate/Send Angiogram data
      run: |
        echo "Generate/Validate/Send Angiogram data"
        bash test-scenarios/test-for-revenue/generate_validate_angiogram_data.sh
        bash test-scenarios/test-for-revenue/send_angiogram_data.sh
        MSGS_SENT=`cat /tmp/msgs_sent`
        MSGS_READ=`cat /tmp/msgs_read`
        echo "total MSGS_SENT=$MSGS_SENT"
        echo "MSGS_SENT=$MSGS_SENT" >> $GITHUB_ENV
        echo "total MSGS_READ=$MSGS_READ"
        echo "MSGS_READ=$MSGS_READ" >> $GITHUB_ENV
    - name: ğŸ‘®  Checkpoint 2 - Data Consistency Check => num_record_sent = num_record_generated
      run: |
        export MSGS_SENT=${{ env.MSGS_SENT }}
        export MSGS_READ=${{ env.MSGS_READ }}
        MSGS_SENT=`cat /tmp/msgs_sent`
        MSGS_READ=`cat /tmp/msgs_read`
        echo "total MSGS_SENT=$MSGS_SENT"
        echo "total MSGS_READ=$MSGS_READ"
        if [ "$MSGS_READ" -eq "$MSGS_SENT" ]; then
          echo "The msgs sent = msgs read are equal."
        else
          echo "The #msgs sent != #records generated !!!!."
          docker compose logs angiogram-api
          exit 3
        fi
    - name: â˜ï¸â€  Checkpoint 3 - Data Consistency Check => check if all recs in kafka
      run: |
        pwd
        MIN_MSG_TO_GET=${{ env.MSGS_READ }}
        KAFKA_CONSUMER_LIFETIME_SEC=30
        python kafka_check.py $KAFKA_CONSUMER_LIFETIME_SEC  ${{ env.HOST_IP }}:9095 gid-$RANDOM ${{ env.TAXI_TOPIC }} $MIN_MSG_TO_GET
    - name: ğŸ”«  Trigger Pipeline Dag
      run: |
        echo "copying required code to airflow..."
        cp create_stats_from_taxi_rides.py dags/
        cp kafka_to_parquet_job.py dags/
        echo "installing deps inside airflow...."
        docker exec -t airflow_scheduler pip3 install --no-cache-dir -r  /opt/airflow/dags/requirements.txt
        bash trigger_dag.sh taxi_revenue_by_car_model_date
        sleep 40
        parq airflow-data/angiogram_data/stats_data/stats.parquet --head 100
    - name: â˜ï¸â€  Checkpoint 4 - Check Dag success ?
      run: |
        RUN_ID=`cat /tmp/angiogram_dag_run_id`
        echo "RUN_ID=$RUN_ID"
        bash check_dag_status.sh taxi_revenue_by_car_model_date $RUN_ID
    - name: âœ…âŒ Final Checkpoint - Angiogram Verification
      run: |
        "echo By running pytest, we are verifying the angiogram based on pipeline output"
        pwd=`pwd`
        export STATS_DATA_PATH=${pwd}/airflow-data/angiogram_data/stats_data
        echo "pipeline output at $STATS_DATA_PATH ...."
        pytest verify_angiogram.py
    - name: ğŸ’šğŸ‰ğŸ’¯ Congratulations !
      run: |
        echo "Congrats! Data Angiogram has been verified ğŸ™Œ"
    - name: ğŸ”¥  Tear down
      run: |
        docker compose down -v || echo "tried to do docker compose down, it errored out. no worries"
    - name: ğŸ””  Slack
      uses: ravsamhq/notify-slack-action@v2
      if: ${{ always() }}
      # for slack webhook url, you need to install the slack app 'INCOMING WEBHOOKS'
      with:
        status: ${{ job.status }} # required
        notification_title: "{workflow} has {status_message}"
        footer: "<{run_url}|View Run> . this is ${{ github.event_name }} run"
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}